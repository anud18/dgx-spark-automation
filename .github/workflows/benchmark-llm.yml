name: Benchmark LLM Model

on:
  workflow_dispatch:
    inputs:
      runner:
        description: 'Runner to use for benchmarking'
        required: false
        type: string
        default: 'spark-1697'
      model:
        description: 'Model to benchmark (e.g., Qwen/Qwen3-4B, or "default" to auto-detect)'
        required: true
        type: string
        default: 'default'
      url_and_port:
        description: 'URL and port of the model server (e.g., 192.168.100.98:8080)'
        required: true
        type: string
        default: '192.168.100.98:8080'
      isl:
        description: 'Input sequence length'
        required: true
        type: string
        default: '64'
      osl:
        description: 'Output sequence length'
        required: true
        type: string
        default: '64'
      concurrency:
        description: 'Number of concurrent requests'
        required: true
        type: string
        default: '10'
      num_requests:
        description: 'Total number of requests to send'
        required: true
        type: string
        default: '20'
      random_seed:
        description: 'Random seed for generating inputs (default: random)'
        required: false
        type: string
        default: 'default'
      output_dir_name:
        description: 'Directory name to save benchmark results'
        required: true
        type: string
        default: 'benchmark-results'

jobs:
  benchmark:
    runs-on: ${{ inputs.runner }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Generate random seed
        id: seed
        run: |
          if [ "${{ inputs.random_seed }}" == "default" ]; then
            SEED=$RANDOM
            echo "Generated random seed: $SEED"
          else
            SEED="${{ inputs.random_seed }}"
            echo "Using provided seed: $SEED"
          fi
          echo "random_seed=$SEED" >> $GITHUB_OUTPUT

      - name: Detect model from v1/models endpoint
        id: model
        run: |
          if [ "${{ inputs.model }}" == "default" ]; then
            echo "Model is set to 'default', querying v1/models endpoint..."
            RESPONSE=$(curl -s "http://${{ inputs.url_and_port }}/v1/models")
            MODEL_NAME=$(echo "$RESPONSE" | grep -o '"id"[[:space:]]*:[[:space:]]*"[^"]*"' | head -1 | sed 's/"id"[[:space:]]*:[[:space:]]*"\([^"]*\)"/\1/')
            if [ -z "$MODEL_NAME" ]; then
              echo "Error: Failed to detect model from v1/models endpoint"
              echo "Response: $RESPONSE"
              exit 1
            fi
            echo "Detected model: $MODEL_NAME"
          else
            MODEL_NAME="${{ inputs.model }}"
            echo "Using provided model: $MODEL_NAME"
          fi
          echo "model_name=$MODEL_NAME" >> $GITHUB_OUTPUT

      - name: Set output directory
        id: output
        run: |
          OUTPUT_DIR="${{ inputs.output_dir_name }}_${{ steps.seed.outputs.random_seed }}"
          echo "output_dir=$OUTPUT_DIR" >> $GITHUB_OUTPUT
          echo "Output directory will be: $OUTPUT_DIR"

      - name: Create output directory
        run: |
          mkdir -p ~/benchmark_result/${{ steps.output.outputs.output_dir }}
          echo "Created output directory: ~/benchmark_result/${{ steps.output.outputs.output_dir }}"

      - name: Run benchmark
        run: |
          echo "=== Benchmark Configuration ==="
          echo "Model: ${{ steps.model.outputs.model_name }}"
          echo "URL and Port: ${{ inputs.url_and_port }}"
          echo "Input Sequence Length: ${{ inputs.isl }}"
          echo "Output Sequence Length: ${{ inputs.osl }}"
          echo "Concurrency: ${{ inputs.concurrency }}"
          echo "Number of Requests: ${{ inputs.num_requests }}"
          echo "Random Seed: ${{ steps.seed.outputs.random_seed }}"
          echo "Output Directory: ${{ steps.output.outputs.output_dir }}"
          echo "=============================="
          echo ""

          docker run --rm  \
            -v ~/benchmark_result:/app \
            nvcr.io/nvidia/ai-dynamo/aiperf:0.3.0 \
            "aiperf profile \
            --model ${{ steps.model.outputs.model_name }} \
            --url ${{ inputs.url_and_port }} \
            --isl ${{ inputs.isl }} \
            --osl ${{ inputs.osl }} \
            --concurrency ${{ inputs.concurrency }} \
            --num-requests ${{ inputs.num_requests }} \
            --num-warmup-requests 1 \
            --artifact-dir ${{ steps.output.outputs.output_dir }} \
            --streaming \
            --random-seed ${{ steps.seed.outputs.random_seed }}"

      - name: Display benchmark results
        if: success()
        run: |
          echo "=== Benchmark Complete ==="
          echo "Results saved to: ~/benchmark_result/${{ steps.output.outputs.output_dir }}"
          echo ""
          if [ -d ~/benchmark_result/${{ steps.output.outputs.output_dir }} ]; then
            echo "Output files:"
            ls -lh ~/benchmark_result/${{ steps.output.outputs.output_dir }}
          fi

      - name: Upload benchmark results
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_number }}
          path: ~/benchmark_result/${{ steps.output.outputs.output_dir }}
          retention-days: 30

      - name: Cleanup on failure
        if: failure()
        run: |
          echo "Benchmark failed. Cleaning up..."
          docker ps -a | grep aiperf || true
